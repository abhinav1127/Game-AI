1. The bot doesn't avoid the radiation in the map because it gets more reward by waiting in its base. Although it loses 20 points of reward every time in radiation, this shorter path allows the robot to attain more reward by just waiting in the base and losing a small amount of points to radiation. To change this, decrease the reward for either the staying in the home base or for being in radiation.

2. The smallest value of enemyDead that the bot is willing to kill the enemy if they cross paths is 1.0. This makes sense; the agent will gain more reward by just executing what it normally does and then killing the robot whenever it encounters it. This is because the agent realizes it gets more reward by remaining in the its safe zone instead of seeking out the enemy to destroy it. 

The smallest value of enemyDead where the bot seeks out the enemy and kills it is 2.0. The reward is now double. This is because the agent will now attain more reward from seeking and killing the enemy as soon as possible. As a result, before it completes any of its objectives, the agent kills the enemy. Killing the enemy early results in the largest amount of points because we get a reward each turn it is dead.

3. The policy (as in the order of objectives) that our agent develops is similar to how it was previously. However, the agent here tends to take more iterations to converge to its final answer previously. The enemy's more random actions makes it so the agent will see the same states much less frequently. This makes it harder for our agent to know which action to take in any given state. As a result, our agent takes longer to learn the ideal policy.